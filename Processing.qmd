---
title: "Processing"
author: "Grant Suchecki"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
jupyter: python3
---

```{python}
# XGBoost — direct run (requires xgboost in this kernel)
import pandas as pd
from pathlib import Path
import joblib
from sklearn.metrics import mean_squared_error
try:
  import xgboost as xgb
  print('xgboost version in-kernel:', xgb.__version__)
except Exception as e:
  print('xgboost not available in-kernel:', e)

p = Path('premodeldatav1.csv')
df = pd.read_csv(p)

features = ['GridPosition','AirTemp_C','TrackTemp_C','Humidity_%','Pressure_hPa','WindSpeed_mps','WindDirection_deg','SOFT','MEDIUM','HARD','INTERMEDIATE','WET','races_prior_this_season']
target = 'DeviationFromAvg_s'

df = df[~df[target].isna()].copy()
train = df[df.get('Season') == 2023].copy()
val = df[df.get('Season') == 2024].copy()
test = df[df.get('Season') == 2025].copy()

if len(train)==0 or len(val)==0 or len(test)==0:
  raise RuntimeError('Seasonal split incomplete; ensure seasons 2023/2024/2025 are present')

X_train = train[features].copy()
X_val = val[features].copy()
X_test = test[features].copy()
y_train = train[target]
y_val = val[target]
y_test = test[target]

# impute medians
for col in features:
  med = X_train[col].median()
  X_train[col] = X_train[col].fillna(med)
  X_val[col] = X_val[col].fillna(med)
  X_test[col] = X_test[col].fillna(med)

if 'xgb' in globals() and hasattr(xgb, 'DMatrix'):
  dtrain = xgb.DMatrix(X_train, label=y_train)
  dval = xgb.DMatrix(X_val, label=y_val)
  params = {'objective':'reg:squarederror','eval_metric':'rmse','learning_rate':0.05,'max_depth':6,'seed':42}
  bst = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dtrain,'train'),(dval,'val')], early_stopping_rounds=50, verbose_eval=50)
  pred_val = bst.predict(xgb.DMatrix(X_val))
  pred_test = bst.predict(xgb.DMatrix(X_test))
  Path('artifacts').mkdir(exist_ok=True)
  bst.save_model('artifacts/xgb_model_untitled_direct.json')
  joblib.dump({'features':features}, 'artifacts/xgb_feature_list_untitled_direct.joblib')
  pd.DataFrame({'y_true': y_val.reset_index(drop=True), 'y_pred': pred_val}).to_csv('artifacts/val_predictions_xgb_untitled_direct.csv', index=False)
  pd.DataFrame({'y_true': y_test.reset_index(drop=True), 'y_pred': pred_test}).to_csv('artifacts/test_predictions_xgb_untitled_direct.csv', index=False)
  print('Direct xgboost training complete — artifacts saved to artifacts/')
else:
  print('xgboost not available in this kernel; use the external helper below to run with conda python')

```

```{python}
# External-run helper: write a script to artifacts/run_xgb_untitled.py and run it with a detected conda python
import os
import subprocess
from pathlib import Path

conda_python = None
if os.environ.get('CONDA_PREFIX'):
  maybe = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'python')
  if os.path.exists(maybe):
    conda_python = maybe
for candidate in ['/opt/anaconda3/bin/python', '/usr/local/anaconda3/bin/python', '/opt/homebrew/bin/python', '/usr/bin/python3']:
  if conda_python is None and os.path.exists(candidate):
    conda_python = candidate

script_path = Path('artifacts/run_xgb_untitled.py')
script_path.parent.mkdir(exist_ok=True)

script = '''import pandas as pd
from pathlib import Path
import xgboost as xgb
from sklearn.metrics import mean_squared_error

p = Path('premodeldatav1.csv')
df = pd.read_csv(p)
features = %s
target = '%s'
df = df[~df[target].isna()].copy()
train = df[df.get('Season') == 2023].copy()
val = df[df.get('Season') == 2024].copy()
test = df[df.get('Season') == 2025].copy()
X_train = train[features].copy()
X_val = val[features].copy()
X_test = test[features].copy()
y_train = train[target]
y_val = val[target]
y_test = test[target]
for col in features:
  med = X_train[col].median()
  X_train[col] = X_train[col].fillna(med)
  X_val[col] = X_val[col].fillna(med)
  X_test[col] = X_test[col].fillna(med)

dtrain = xgb.DMatrix(X_train, label=y_train)
dval = xgb.DMatrix(X_val, label=y_val)
dtest = xgb.DMatrix(X_test, label=y_test)
params = {'objective':'reg:squarederror','eval_metric':'rmse','learning_rate':0.05,'max_depth':6,'seed':42}
bst = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dtrain,'train'),(dval,'val')], early_stopping_rounds=50, verbose_eval=50)
Path('artifacts').mkdir(exist_ok=True)
bst.save_model('artifacts/xgb_model_untitled_external.json')
pd.DataFrame({'y_true': y_val.reset_index(drop=True), 'y_pred': bst.predict(dval)}).to_csv('artifacts/val_predictions_xgb_untitled_external.csv', index=False)
pd.DataFrame({'y_true': y_test.reset_index(drop=True), 'y_pred': bst.predict(dtest)}).to_csv('artifacts/test_predictions_xgb_untitled_external.csv', index=False)
# attempt to save SHAP-like contributions (XGBoost pred_contribs)
try:
  shap_val = bst.predict(dval, pred_contribs=True)
  shap_test = bst.predict(dtest, pred_contribs=True)
  cols = features + ['bias']
  import numpy as _np
  pd.DataFrame(shap_val, columns=cols).to_csv('artifacts/shap_val_xgb_untitled_external.csv', index=False)
  pd.DataFrame(shap_test, columns=cols).to_csv('artifacts/shap_test_xgb_untitled_external.csv', index=False)
  pd.Series(_np.abs(shap_val).mean(axis=0), index=cols).sort_values(ascending=False).to_csv('artifacts/shap_summary_val_xgb_untitled_external.csv')
  print('Saved SHAP-like pred_contribs to artifacts/')
except Exception as _e:
  print('Could not compute pred_contribs in external script:', _e)

print('External xgboost run complete')
'''

script = script % (repr(['GridPosition','AirTemp_C','TrackTemp_C','Humidity_%','Pressure_hPa','WindSpeed_mps','WindDirection_deg','SOFT','MEDIUM','HARD','INTERMEDIATE','WET','races_prior_this_season']), 'DeviationFromAvg_s')
script_path.write_text(script)

if conda_python and os.path.exists(conda_python):
  print('Running external script with', conda_python)
  rc = subprocess.call([conda_python, str(script_path)])
  print('external script exit code', rc)
else:
  print('No conda/python detected; run the script manually with your conda python:')
  print('/path/to/conda/python', script_path)

```

```{python}
# Merge AvgPitStopTime into premodeldatav1.csv (robust coercion)
import pandas as pd
from pathlib import Path
import numpy as np

P = Path('premodeldatav1.csv')
RR = Path('race_results_2023_2024_2025_with_pitstops.csv')
RR2 = Path('race_results_2023_2024_2025.csv')

df = pd.read_csv(P)

# Try to load a race_results file that already contains AvgPitStopTime
if RR.exists():
  rr = pd.read_csv(RR)
elif RR2.exists():
  rr = pd.read_csv(RR2)
else:
  rr = None

# Determine join keys
season_col = 'Season' if 'Season' in df.columns else None
round_col = 'Round' if 'Round' in df.columns else ('RoundNumber' if 'RoundNumber' in df.columns else None)

# Prefer driver matching by DriverNumber if present, else Abbreviation/Driver
if 'DriverNumber' in df.columns:
  left_on = ['Season', 'Round', 'DriverNumber']
  if rr is not None and 'DriverNumber' not in rr.columns:
    # fallback to abbreviation matching if race_results uses different naming
    left_on = ['Season', 'Round', 'Abbreviation']
else:
  left_on = ['Season', 'Round', 'Abbreviation'] if 'Abbreviation' in df.columns else ['Season', 'Round', 'Driver']

# Build right-hand table with AvgPitStopTime if available
if rr is not None and 'AvgPitStopTime' in rr.columns:
  # normalize right keys
  if 'RoundNumber' in rr.columns and 'Round' not in rr.columns:
    rr = rr.rename(columns={'RoundNumber':'Round'})
  # choose a driver id column
  if 'DriverNumber' in rr.columns:
    right_on = ['Season','Round','DriverNumber']
  elif 'Abbreviation' in rr.columns:
    right_on = ['Season','Round','Abbreviation']
  else:
    # try Driver / BroadcastName
    cand = 'Driver' if 'Driver' in rr.columns else ('BroadcastName' if 'BroadcastName' in rr.columns else None)
    right_on = ['Season','Round',cand] if cand else ['Season','Round']

  # keep only join cols and AvgPitStopTime
  right = rr.copy()
  right = right[[c for c in right_on if c is not None and c in right.columns] + ['AvgPitStopTime']]
  # coerce season/round to numeric for safe merge
  for c in ['Season','Round']:
    if c in right.columns:
      right[c] = pd.to_numeric(right[c], errors='coerce')
    if c in df.columns:
      df[c] = pd.to_numeric(df[c], errors='coerce')

  merged = df.merge(right, how='left', left_on=[c for c in left_on if c in df.columns], right_on=[c for c in right_on if c in right.columns], suffixes=('','_r'))
  # prefer existing AvgPitStopTime column if present
  if 'AvgPitStopTime' not in merged.columns:
    merged['AvgPitStopTime'] = merged.get('AvgPitStopTime_r')
  else:
    # fill NaNs in main from right
    merged['AvgPitStopTime'] = merged['AvgPitStopTime'].fillna(merged.get('AvgPitStopTime_r'))
  df = merged.drop(columns=[c for c in merged.columns if c.endswith('_r')])

# Coerce/fill missing AvgPitStopTime values: per-driver mean, then overall median
if 'AvgPitStopTime' not in df.columns:
  df['AvgPitStopTime'] = np.nan

if df['AvgPitStopTime'].isna().any():
  # compute per-driver mean from whatever values exist
  key_driver = 'DriverNumber' if 'DriverNumber' in df.columns else ('Abbreviation' if 'Abbreviation' in df.columns else 'Driver')
  per_driver = df.groupby(key_driver)['AvgPitStopTime'].transform('mean')
  df['AvgPitStopTime'] = df['AvgPitStopTime'].fillna(per_driver)
  # final fallback: overall median of non-null pit times
  med = df['AvgPitStopTime'].median()
  if np.isnan(med):
    # if still NaN (no pit info anywhere), set to 0.0 to avoid breaking numeric pipelines
    med = 0.0
  df['AvgPitStopTime'] = df['AvgPitStopTime'].fillna(med)

# Save a backup and overwrite CSV
bak = str(P) + '.bak'
if not Path(bak).exists():
  P.rename(bak)
  print('Backed up original CSV to', bak)
else:
  print('Backup already exists at', bak)
df.to_csv(P, index=False)
print('Wrote merged AvgPitStopTime into', P)

```

```{python}
# Expanded hyperparameter tuning (randomized search) block
import numpy as np
import pandas as pd
from pathlib import Path
import joblib
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
import os

P = Path('premodeldatav1.csv')
df = pd.read_csv(P)

features = ['GridPosition','AirTemp_C','TrackTemp_C','Humidity_%','Pressure_hPa','WindSpeed_mps','WindDirection_deg','SOFT','MEDIUM','HARD','INTERMEDIATE','WET','races_prior_this_season']
target = 'DeviationFromAvg_s'

df = df[~df[target].isna()].copy()
train = df[df.get('Season') == 2023].copy()
val = df[df.get('Season') == 2024].copy()
test = df[df.get('Season') == 2025].copy()

if len(train)==0 or len(val)==0:
  raise RuntimeError('Seasonal split incomplete; ensure seasons 2023/2024/2025 are present')

X_train = train[features].copy()
X_val = val[features].copy()
y_train = train[target]
y_val = val[target]

# simple imputation
for col in features:
  med = X_train[col].median()
  X_train[col] = X_train[col].fillna(med)
  X_val[col] = X_val[col].fillna(med)

# If xgboost available in kernel, run randomized tuning inline
try:
  import xgboost as xgb
  from scipy.stats import randint
  param_dist = {
    'max_depth': randint(3,10),
    'learning_rate': np.linspace(0.01,0.2,20),
    'n_estimators': randint(50,500),
    'subsample': np.linspace(0.5,1.0,6),
    'colsample_bytree': np.linspace(0.5,1.0,6),
    'reg_alpha': np.linspace(0,1.0,11),
    'reg_lambda': np.linspace(0,1.0,11)
  }

  model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, verbosity=0)
  n_trials = 20
  rs = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_trials, scoring='neg_root_mean_squared_error', cv=3, n_jobs=1, verbose=2, random_state=42)
  rs.fit(X_train, y_train)
  print('Best params:', rs.best_params_)
  best = rs.best_estimator_
  Path('artifacts').mkdir(exist_ok=True)
  joblib.dump(best, 'artifacts/xgb_best_inline.joblib')
  # evaluate on validation
  pred_val = best.predict(X_val)
  pd.DataFrame({'y_true': y_val.reset_index(drop=True), 'y_pred': pred_val}).to_csv('artifacts/val_predictions_xgb_tuned_inline.csv', index=False)
  print('Tuning complete; saved model and val predictions to artifacts/')
except Exception as e:
  print('Could not run inline tuning (xgboost/scipy missing). Will write an external tuning script to artifacts and attempt to run with detected conda python. Error:', e)
  # write external tuning script
  script = Path('artifacts/run_xgb_tune_randomized.py')
  script.write_text(f"""import pandas as pd
import numpy as np
from pathlib import Path
import joblib
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error
from scipy.stats import randint

p = Path('premodeldatav1.csv')
df = pd.read_csv(p)
features = {repr(features)}
target = '{target}'
df = df[~df[target].isna()].copy()
train = df[df.get('Season') == 2023].copy()
val = df[df.get('Season') == 2024].copy()
X_train = train[features].copy()
X_val = val[features].copy()
y_train = train[target]
y_val = val[target]
for col in features:
  med = X_train[col].median()
  X_train[col] = X_train[col].fillna(med)
  X_val[col] = X_val[col].fillna(med)

param_dist = {{
  'max_depth': randint(3,10),
  'learning_rate': np.linspace(0.01,0.2,20),
  'n_estimators': randint(50,500),
  'subsample': np.linspace(0.5,1.0,6),
  'colsample_bytree': np.linspace(0.5,1.0,6),
  'reg_alpha': np.linspace(0,1.0,11),
  'reg_lambda': np.linspace(0,1.0,11)
}}

model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, verbosity=0)
rs = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, scoring='neg_root_mean_squared_error', cv=3, n_jobs=1, verbose=2, random_state=42)
rs.fit(X_train, y_train)
best = rs.best_estimator_
Path('artifacts').mkdir(exist_ok=True)
joblib.dump(best, 'artifacts/xgb_best_external.joblib')
pd.DataFrame({'y_true': y_val.reset_index(drop=True), 'y_pred': best.predict(X_val)}).to_csv('artifacts/val_predictions_xgb_tuned_external.csv', index=False)
print('External tuning complete')
""")
  # attempt to run with conda python if available
  conda_python = None
  if os.environ.get('CONDA_PREFIX'):
    maybe = os.path.join(os.environ['CONDA_PREFIX'], 'bin', 'python')
    if os.path.exists(maybe):
      conda_python = maybe
  for candidate in ['/opt/anaconda3/bin/python', '/usr/local/anaconda3/bin/python', '/opt/homebrew/bin/python', '/usr/bin/python3']:
    if conda_python is None and os.path.exists(candidate):
      conda_python = candidate
  if conda_python:
    print('Running external tuning with', conda_python)
    import subprocess
    rc = subprocess.call([conda_python, str(script)])
    print('external tuning exit code', rc)
  else:
    print('No conda/python detected; run the script manually with your conda python:')
    print('/path/to/conda/python', script)

```

```{python}
# Results & SHAP loader — compute metrics and summarize SHAP/pred_contribs
import pandas as pd
from pathlib import Path
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

art = Path('artifacts')
if not art.exists():
  raise RuntimeError('No artifacts/ directory found; run the training first')

# load data
df = pd.read_csv('premodeldatav1.csv')
df = df[~df['DeviationFromAvg_s'].isna()].copy()
train = df[df.get('Season') == 2023].copy()
val = df[df.get('Season') == 2024].copy()
test = df[df.get('Season') == 2025].copy()
features = ['GridPosition','AirTemp_C','TrackTemp_C','Humidity_%','Pressure_hPa','WindSpeed_mps','WindDirection_deg','SOFT','MEDIUM','HARD','INTERMEDIATE','WET','races_prior_this_season']

for col in features:
  med = train[col].median()
  val[col] = val[col].fillna(med)
  test[col] = test[col].fillna(med)

# try to load in-kernel booster first
rmse_val = rmse_test = None
mae_val = mae_test = None
r2_val = r2_test = None

model_loaded = False
try:
  import xgboost as xgb
  # if a booster object exists in the namespace, use it
  if 'bst' in globals():
    booster = globals()['bst']
    dval = xgb.DMatrix(val[features])
    dtest = xgb.DMatrix(test[features])
    pred_val = booster.predict(dval)
    pred_test = booster.predict(dtest)
    model_loaded = True
  else:
    # try to load saved booster
    for p in ['artifacts/xgb_model_untitled_direct.json','artifacts/xgb_model_untitled_external.json','artifacts/xgb_model.json']:
      if Path(p).exists():
        booster = xgb.Booster()
        booster.load_model(p)
        dval = xgb.DMatrix(val[features])
        dtest = xgb.DMatrix(test[features])
        pred_val = booster.predict(dval)
        pred_test = booster.predict(dtest)
        model_loaded = True
        print('Loaded booster from', p)
        break
except Exception as e:
  print('xgboost not usable in-kernel for metrics loading:', e)

if not model_loaded:
  # try to read prediction files written by external script
  pv = art / 'val_predictions_xgb_untitled_external.csv'
  pt = art / 'test_predictions_xgb_untitled_external.csv'
  if pv.exists() and pt.exists():
    pred_val = pd.read_csv(pv)['y_pred'].values
    pred_test = pd.read_csv(pt)['y_pred'].values
    print('Loaded predictions from artifacts/')
  else:
    raise RuntimeError('No model or prediction artifacts found. Run the training first.')

# compute metrics
def compute_metrics(y_true, y_pred):
  return {
    'rmse': mean_squared_error(y_true, y_pred, squared=False),
    'mae': mean_absolute_error(y_true, y_pred),
    'r2': r2_score(y_true, y_pred)
  }

metrics_val = compute_metrics(val['DeviationFromAvg_s'].values, pred_val)
metrics_test = compute_metrics(test['DeviationFromAvg_s'].values, pred_test)

print('Validation metrics: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}'.format(**metrics_val))
print('Test metrics:       RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}'.format(**metrics_test))

# save metrics to artifacts
pd.Series(metrics_val).to_json(art / 'metrics_val_xgb.json')
pd.Series(metrics_test).to_json(art / 'metrics_test_xgb.json')

# load SHAP-like pred_contribs if present
shap_candidates = [art / 'shap_val_xgb_untitled_external.csv', art / 'shap_test_xgb_untitled_external.csv', art / 'shap_val_xgb.csv']
if (art / 'shap_val_xgb_untitled_external.csv').exists():
  sv = pd.read_csv(art / 'shap_val_xgb_untitled_external.csv')
  st = pd.read_csv(art / 'shap_test_xgb_untitled_external.csv')
  print('Loaded SHAP-like pred_contribs from artifacts; saving brief summaries')
  # mean absolute contribution per feature
  def shap_summary(df_shap):
    return df_shap.abs().mean().sort_values(ascending=False)
  shap_summary(sv).to_csv(art / 'shap_summary_val_xgb.csv')
  shap_summary(st).to_csv(art / 'shap_summary_test_xgb.csv')
  print('Saved shap summaries to artifacts/')
else:
  print('No SHAP/pred_contrib files found in artifacts')

```
```
